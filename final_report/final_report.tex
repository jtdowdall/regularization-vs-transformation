\documentclass[letterpaper, 12pt]{article}
%\setcounter{secnumdepth}{0}
\usepackage{fontspec}
\usepackage{ctable}
\defaultfontfeatures{Ligatures=TeX}
\usepackage[small,sf,bf]{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[empty]{fullpage}
\setlength{\parskip}{\medskipamount}
\usepackage{nopageno}

\usepackage{caption}
\captionsetup[table]{skip=10pt}

\usepackage{rotating}
\usepackage{rotfloat}

\usepackage{listings}
%\newfontfamily\Consolas{DejaVu Sans Mono}
%\lstset{basicstyle=\footnotesize\Consolas}

\setmainfont{CMU Serif}

\raggedbottom
\raggedright

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "r" column type
%\newcolumntype{C}{ >{\centering\arraybackslash} m{6cm} }
%\newcolumntype{D}{ >{\centering\arraybackslash} m{4.5cm} }

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\begin{document}

\renewcommand{\arraystretch}{1.1}

\title{Report}
\author{Eric Alan Wayman}
\date{Due May 4, 5 PM}
\maketitle

\section{Introduction}

\subsection{Motivation}

\subsection{Literature review}

\subsection{Problem statement}

\section{Approach}

$\ell_1$ regularization (sparsity) vs. $\ell_2$ regularization: $\ell_1$ regularization will work better when a small subset of the features are significant. l2 regularization will work better when many of the features are significant.

linear vs rbf kernel: the linear kernel will work fine when the data is already linearly separable. the rbf kernel will help when the data is not linearly separable.

svm rbf and svm linear will perform similarly if there is no need to transform data to higher dimensional space. svm rbf will outperform svm linear if data is not linearly separable in original space.

SVM rbf vs RBF logistic: SVM will outperform logistic when generalization is important (large margin/sparsity of data points that comes from support vectors is not there for RBF logistic so it gives poor results on leukimia (poor generalization)).

\subsection{Logistic regression with $\ell_1$ and $\ell_2$ regularization}

Logistic regression is a discriminative classifier. It corresponds to a binary classification model:

\begin{equation*}
  p(y | \vect{x}, \vect{w}) = \text{Ber}(y | \text{sigm}(\vect{w}^T\vect{x}))
\end{equation*}

(Murphy p. 245) where sigm is the sigmoid function. If the possible values of $y$ are either $-1$ or $+1$, then $p(y = 1) = 1 / (1 + \exp(-\vect{w}^T\vect{x}))$ and $p(y = -1) = 1 / (1 + \exp(\vect{w}^T\vect{x}))$, so the negative log-likelihood, which equals negative one times the error of the model and which is therefore to be maximized, is

\begin{equation*}
  NLL(\vect{w}) = \log(1 + \exp(-y_i \vect{w}^T\vect{x}))
\end{equation*}

(Murphy p. 245). There is not a closed-form solution for the MLE of $\vect{w}$, so it must estimated by an optimization algorithm.

L2 regularization is achieved by adding the term $\frac{\lambda}{2} {\norm{w}_2}^2$ to $NLL(\vect{w})$ above, giving

\begin{equation*}
  NLL(\vect{w}, \lambda) = \log(1 + \exp(-y_i \vect{w}^T\vect{x})) + \frac{\lambda}{2} {\norm{\vect{w}}_2}^2
\end{equation*}

L1 regularization is achieved by adding the term $\lambda {\norm{\vect{w}}_1}^2$ where $\norm{\vect{w}}_1 = \sum_{i=1}^{c} |\vect{w}_i|$ (Theodoridis p. 404), so

\begin{equation*}
  NLL(\vect{w}, \lambda) = \log(1 + \exp(-y_i \vect{w}^T\vect{x})) + \lambda {\norm{\vect{w}}_1}
\end{equation*}

\subsubsection{The effects of $\ell_1$ and $\ell_2$ regularization}

When the $\ell_2$ regularization term is included, maximizing the NLL function with respect to $\vect{w}$ and $\lambda$ tries to reduce the norm of $\vect{w}$ (the vector of parameters) while at the same time minimizing the error given by the log-likelihood cost function (maximizing the negative of this function). This helps prevent overfitting: by restricting the $\ell_2$ norm of $\vect{w}$, the ``complexity'' of the model is restricted, so it is prevented from ``learning too much about the idiosyncrasies of the specific training data set'' (Theodoridis, p.74).

If only a few features contain significant information and there are a large number of features, the ``true'' model generating the data will have the coefficients of most components of $\vect{w}$ equal to zero. Therefore it 

The following figure (Figure 1, taken from Theodoridis p. 406, Figure 9.2) shows the relationship between a given component $\theta$ of the parameter vector $\vect{\theta}$ (what we call $\vect{w}$) and its contribution to $\norm{\vect{\theta}}_p$, $|\theta|^p$, for given levels of $p$. For $\ell_p$ norms with $p \geq 1$, components $\theta$ with larger $|\theta|^p$ give a larger contribution to the norm, so assuming for example's sake that two components $\theta_1$ and $\theta_2$ have the same effect on the fit of the model and $|\theta_1|^p > |\theta_2|^p > 1$, the minimization will try to reduce the size of $\theta_1$ more than $\theta_2$. Conversely, for $p > 1$, any $\theta_j$ with $|\theta_j|^p < 1$ will not have its size reduced very much at all, irrespective of the amount to which it contributes to minimizing the error of the model.

However, for $p = 1$, even components $\theta_j$ with $|\theta_j|^1 < 1$ will have the regularization applied to them. Therefore irrespective of the size of a true $\theta_j$, the regularization will force $\theta_j$ to 0 if it does not contribute to minimizing model error.

(The above discussion was based Theodoridis, p. 406-407)

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{figure1.jpg}
\caption{A simple caption \label{overflow}}
\end{figure}

\subsection{Kernels: linear vs RBF}

Kernels were discussed in lecture. A Mercer kernel is a kernel whose Gram matrix

\begin{equation*}
  \vect{K} = \begin{pmatrix}
    \kappa(\vect{x}_1, \vect{x}_1) & \cdots & \kappa(\vect{x}_1, \vect{x}_N) \\
    & \vdots & \\
    \kappa(\vect{x}_N, \vect{x}_1) & \cdots & \kappa(\vect{x}_N, \vect{x}_N) \\    
  \end{pmatrix}
\end{equation*}

is positive semi-definite for any set of inputs $\{\vect{x}_i\}_{i=1}^N$ (Murphy p. 481). For any Mercer kernel there exists a function $\vect{\phi}: \mathcal{X} \rightarrow \mathbb{R}^D$ for which then $K(\vect{x}, \vect{x}^\prime) = \vect{\phi}(\vect{x})^T\vect{\phi}(\vect{x})$. Note that $D$ can be infinite, as explained in the section ``SVM and RBF kernel relationship explanation.''

In this project we use two kernels, linear kernels and the RBF kernel, both of which are Mercer kernels. The kernels will be used in this project as transformations of data to be input to classifiers which produce a linear decision boundary (if transformed data is input to a classifier, the resulting decision boudary will be linear in that transformed space).

Note that usually it is hard to derive the feature vector $\vect{\phi}(\vect{x})$ from a Kernel $\kappa(\vect{x}, \vect{x}^\prime)$, but the reverse is not difficult for a Mercer kernel since $\kappa(\vect{x}, \vect{x}^\prime) = \vect{\phi}(\vect{x})$.

The linear kernel is $\kappa(\vect{x}, \vect{x}^\prime) = \vect{x}^T\vect{x}^\prime$, which corresponds to the case where $\vect{\phi}(\vect{x}) = \vect{x}$, so $\vect{\phi}(\vect{x})$ takes points in $\mathcal{X}$ to $\mathcal{X}$. This kernel is useful in the case where the decision boundary is linear in the original feature space, so transforming them to a higher-dimensional feature space is not necessary (Murphy, p. 482).

The RBF kernel is defined as follows:

\begin{equation*}
  K(\vect{x}, \vect{x}^\prime) = \exp\left(-\gamma \norm{\vect{x} - \vect{x}^\prime}\right)
\end{equation*}

As noted above, the $D$ in $\vect{\phi}(\vect{x}): \mathcal{X} \rightarrow \mathbb{R}^D$ is infinite in the case of the RBF kernel. To understand the transformation, following Abu-Mostafa et al. (p. 8-37), let $\gamma = 1$ and $\vect{x}$ be a scalar. Then

\begin{align*}
  K(x, x^\prime) & = \exp\left(-\norm{x - x^\prime}^2\right) \\
  & = \exp\left(-(x)^2\right) \cdot \exp\left(2xx^\prime\right) \cdot \exp(-\left(x^\prime\right)^2) \\
  & = \exp\left(-(x)^2\right) \cdot \left(\sum_{k=0}^{\infty} \frac{2^k(x)^k\left(x^\prime\right)^k}{k!}\right) \cdot \exp\left(-\left(x^\prime\right)^2\right)
\end{align*}

Defining

\begin{equation*}
  \vect{\phi}(x) = \exp(-x^2) \cdot \left(1, \sqrt{\frac{2^1}{1!}}x, \sqrt{\frac{2^1}{2!}}x^2, \sqrt{\frac{2^1}{3!}}x^3, \ldots \right)
\end{equation*}

we see that $K(x, x^\prime) = \vect{\phi}(x)^T \vect{\phi}(x)$. The right hand side is an inner product in an infinite-dimensional feature space, which shows that the $D$ in the range of $K$ can be infinite.

\subsubsection{The ``kernel trick''}

If it is difficult to compute $\vect{\phi}(\vect{x})^T \vect{\phi}(\vect{x})$, instead we can compute $K(\vect{x}, \vect{x}^\prime)$ in the original $\mathcal{X}$ space since the results are equal. This will be further explained where it appears later.

\subsection{SVMs}

The following table summarizes the combinations of model fitting strategies and data transformations used in this project. Each column indicates a different model fitting strategy while each row indicates a type of data transformation. Each cell indicates the classifier that was used in conjuction with the fitting strategy and data transformation. Note that the model used for any particular combination is deterministic: in other words, the desired model fitting strategy and data transformation indicate a model choice.

\begin{center}
  \begin{tabular}{l|c|c|c|r|}
         & loss & loss + $\ell_1$ & loss + $\ell_2$ & few data pts \\
\hline
  linear &         & log reg  & log reg  & SVM \\
\hline
  RBF    & log reg &          &          & SVM \\
\hline
\end{tabular}
\end{center}

\begin{description}
\item[loss:] loss function with no regularization term \\
\item[loss + $\ell_1$:] loss function with $\ell_1$ regularization term \\
\item[loss + $\ell_2$:] loss function with $\ell_2$ regularization term
\item[few data pts] model attempts to utilize small subset of of training data in parameter estimation 
\end{description}

\subsection{Kernelized logistic regression using RBF kernel}

\subsection{SVM with linear and RBF kernels}

\subsection{SVM and RBF kernel relationship explanation}

kernel trick

sparsity

large margin



\subsubsection{Logistic regression}

MLE:

Therefore

\begin{equation*}
  p(y = 1) = \frac{1}{1 + \exp\left(-\vect{w}^T\vect{x}\right)}
\end{equation*}

and likewise $p(y = -1) = \frac{1}{1 + \exp\left(\vect{w}^T\vect{x}\right)}$ (Murphy p. 246). If $\vect{x}$ is replaced with $\vect{\phi}(\vect{x})$, this becomes 

\begin{equation*}
  p(y = 1) = \frac{1}{1 + \exp\left(-\vect{w}^T\vect{\phi}(\vect{x})\right)}
\end{equation*}

Switching for a moment to SVMs, SVMs have a loss function called hinge loss, which is of the form $L_\text{hinge}(y, \eta) = \max(0, 1 - y \eta) = (1 - y\eta)_{+}$ where $\eta = f(\vect{x})$ is the ``confidence'' (not necessarily a probability) in choosing label $y = 1$ (Murphy p. 499).  

The optimal $f(\vect{x})$ in fitting an SVM is of the form $f(\vect{x}) = \sum_{i=1}^{n} \alpha_i K(\vect{x}, \vect{x}_i^\prime)$ (Zhu and Hastie, p.2).

Since the negative log-likelihood (NLL) for logistic regression has a similar shape to the NLL of the SVM, replacing the NLL of the SVM with the NLL of the logistic regression gives roughly the same solution (Zhu and Hastie, p. 2). Then for a Mercer kernel, the interpretation of the probability $p(\vect{x})$ (which equals $P(y = 1 | \vect{X} = \vect{x})$, Lin 2002) is

\begin{align*}
  p(\vect{x}) & = \frac{e^{f(\vect{x})}}{1 + e^{f(\vect{x})}} = \frac{1}{1 + \exp({-f(\vect{x})})} & \\
  & = \frac{1}{1 + \exp(-\sum_{i=1}^{n} \alpha_i K(\vect{x}, \vect{x}_i^\prime))} & \text{plugging in the optimal solution} \\
  & = \frac{1}{1 + \exp(-\sum_{i=1}^{n} \alpha_i \vect{\phi}(\vect{x}_i) \cdot \vect{\phi}(\vect{x}))} & \text{using the kernel trick} \\
  & = \frac{1}{1 + \exp(-\vect{w}^T \vect{\phi}(\vect{x}))} &
\end{align*}

where the last step is by defining $\vect{w} = \sum_{i} \alpha_i \vect{\phi}(\vect{x}_i)$ is the weighted sum of transformed support vectors. The last two steps here were taken from Guestrin (2007).

\section{Datasets}

\section{Experimental analysis}

\section{Findings}

\section{Summary and future work}

\section{References}

Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin. 2012. \emph{Learning with data}, e-Chapter 8 (``Support vector machines'') AMLBook.

Guestrin, Carlos. 2007. ``Support vector machines.'' Lecture slides for ``Machine Learning â€“ 10701/15781`` at Carnegie Mellon University. 

Lin, Yi. 2002. ``Support vector machines and the Bayes rule in classification.'' Data Mining and Knowledge Discovery (6): 259â€“275.

Murphy, Kevin. 2012. \emph{Machine learning: a probabilistic perspective}. MIT Press: Cambridge, MA.

Theodoridis, Sergios. 2015. \emph{Machine learning: a Bayesian and optimization perspective}. Elsevier: London.

Zhu, Ji and Trevor Hastie. 2004. ``Kernel logistic regression and the import vector machine.'' Preprint.

\end{document}
