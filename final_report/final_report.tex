\documentclass[letterpaper, 11pt]{article}
%\setcounter{secnumdepth}{0}
\usepackage{fontspec}
\usepackage{ctable}
\defaultfontfeatures{Ligatures=TeX}
\usepackage[small,sf,bf]{titlesec}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[empty]{fullpage}
\setlength{\parskip}{\medskipamount}
\usepackage{nopageno}

\usepackage{caption}
\captionsetup[table]{skip=10pt}

\usepackage{rotating}
\usepackage{rotfloat}

\usepackage{listings}
%\newfontfamily\Consolas{DejaVu Sans Mono}
%\lstset{basicstyle=\footnotesize\Consolas}

\setmainfont{CMU Serif}

\raggedbottom
\raggedright

\providecommand{\e}[1]{\ensuremath{\times 10^{#1}}}
\newcolumntype{L}{>{$}l<{$}} % math-mode version of "l" column type
\newcolumntype{C}{>{$}c<{$}} % math-mode version of "c" column type
\newcolumntype{R}{>{$}r<{$}} % math-mode version of "r" column type
%\newcolumntype{C}{ >{\centering\arraybackslash} m{6cm} }
%\newcolumntype{D}{ >{\centering\arraybackslash} m{4.5cm} }

\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}
\newcommand{\vect}[1]{\boldsymbol{#1}}

\makeatletter
\renewcommand*\env@matrix[1][*\c@MaxMatrixCols c]{%
  \hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{#1}}
\makeatother

\begin{document}

\renewcommand{\arraystretch}{1.1}

\title{Regularization and transformation:\\CSE802 Project}
\author{Jonny Dowdall, James Peterkin II, Eric Alan Wayman}
\date{Submitted May 4, 2018}
\maketitle

\section{Introduction}

\subsection{Motivation}

The fact that the performance of classifiers can be improved through methods that 1) impose some form of regularization on the estimated parameters, or 2) exploit useful transformations of the feature space, suggests that understanding of the intracies and interrelationships between these techniques is an important part of any practitioner's toolbox in the field of pattern recognition and machine learning. Our project seeks to build such an understanding and to test that understanding against the application of the techniques to real-world datasets.

The basic idea of regularization is that when fitted to a particular training data set models may learn the idiosyncracies of that data set (overfitting) and that will be reflected in poorer performance on an unseen test data set compared to the performance of the average of a series of models fit to various training sets. Regularization simulates this procedure by restricting the magnitude of components of the parameter vector, forcing the fitted model to be more ``smooth'' than if the parameter vector were unrestricted. Overfitting can also be addressed through sparsity-encouraging regularization ($\ell_1$ regularization, which increases the ``signal to noise ratio'' by encouraging the parameter coefficients of features that do not contribute to reducing model error to go to zero). 

Underfitting occurs when the model is too simple for the data. Since the models in this project produce linear decision boundaries, this will occur when features are not linearly separable in the original feature space. Transforming the data to a higher-dimensional feature space, in this project demonstrated by the use of the RBF kernel, may allow the data to become linearly separable in that feature space. Through the use of the kernel trick, the model can be fit in the higher-dimensional space through performing calculations in the original feature space, and the resulting decision boundary visualized in the original feature space.

Overfitting can also be addressed through fitting the data using a subset of the training points, and by choosing a linear boundary that separates the data points with as large a ``margin'', or distance from the closest points to the boundary, as possible. Both of these qualities are characteristics of the Support Vector Machine (SVM).

Another classifier, logistic regression, can be used in conjunction with the $\ell_1$ and $\ell_2$ norms for regularization, as well as with transformed data (kernelized logistic regression). The details of all these techniques will be explained in sections to follow.

\subsection{Literature review}

Regularization was first proposed by Tychonoff (Theodoridis 2015, 72) in 1977 (Tychonoff and Arsenin 1977). $\ell_1$ and $\ell_2$ regularization are common techniques described in many machine learning textbooks, for example Murphy 2015.

Logistic regression, the first of the two classifiers that were used in this project, was invented by David Cox in 1958 (Cox 1958).

According to Wikipedia, kernel classifiers were likely first mentioned in the 1960s (Aizerman et al. 1964) and gained widespread attention due to the introduction of the support vector machine (SVM) in the 1990s due to the SVM's competitive performance on tasks such as handwriting recognition. This SVM, which uses the ``kernel trick'' to be described below was invented by Boser et al. in 1992 (Boser et. al 1992).

The usage of kernels with logistic regression is demonstrated in Zhu and Hastie (2002).

The techniques explained and used in this project are widely used in pattern classification (for example see).

\subsection{Problem statement}

To explain how techniques of regularization and data transformation can be used in conjunction with the binary classifers logistic regression and SVM, use this understanding to hypothesize regarding the performance of the various techniques on datasets with differing characteristics, and test these hypotheses on real-world datsets. Our hypotheses are as follows:

On datasets which are linearly separable, if most of the features are useful towards classification we expect logistic regression with $\ell_2$ regularization and SVM with a linear kernel to perform well. If only a few of the features are useful towards classification on such a dataset, we expect logistic regression with $\ell_1$ regularization to perform better.

On datasets which are not linearly separable, if most of the features are useful towards classification we expect the SVM with an RBF kernel to perform well. If only a few of the features are useful towards classification on such a dataset, we expect logistic regression with an RBF kernel to perform better.

\section{Approach}

\subsection{Logistic regression with $\ell_1$ and $\ell_2$ regularization}

Logistic regression is a discriminative classifier. It corresponds to a binary classification model:

\begin{equation*}
  p(y | \vect{x}, \vect{w}) = \text{Ber}(y | \text{sigm}(\vect{w}^T\vect{x}))
\end{equation*}

(Murphy 2012, 245) where sigm is the sigmoid function. If the possible values of $y$ are either $-1$ or $+1$, then $p(y = 1) = 1 / (1 + \exp(-\vect{w}^T\vect{x}))$ and $p(y = -1) = 1 / (1 + \exp(\vect{w}^T\vect{x}))$. We minimize the error by maximizing the negative log-likelihood:

\begin{equation*}
  NLL(\vect{w}) = \log(1 + \exp(-y_i \vect{w}^T\vect{x}))
\end{equation*}

(Murphy 2012, 245). There is not a closed-form solution for the MLE of $\vect{w}$, so it must be estimated by an optimization algorithm. However, we must often design constraints to prevent the parameters from overfitting the training data and losing generality.

\subsubsection{The effects of $\ell_1$ and $\ell_2$ regularization}

When the $\ell_2$ regularization term is included, maximizing the NLL function with respect to $\vect{w}$ and $\lambda$ tries to reduce the norm of $\vect{w}$ (the vector of parameters) while at the same time minimizing the error given by the log-likelihood cost function (maximizing the negative of this function). This helps prevent overfitting: by restricting the $\ell_2$ norm of $\vect{w}$, the ``complexity'' of the model is restricted, so it is prevented from ``learning too much about the idiosyncrasies of the specific training data set'' (Theodoridis 2015, 74).

If only a few features contain significant information and there are a large number of features, the ``true'' model generating the data will have the coefficients of most components of $\vect{w}$ equal to zero. Therefore it 

The following figure (Figure 1, taken from Theodoridis 2015, 406, Figure 9.2) shows the relationship between a given component $\theta$ of the parameter vector $\vect{\theta}$ (what we call $\vect{w}$) and its contribution to $\norm{\vect{\theta}}_p$, $|\theta|^p$, for given levels of $p$. For $\ell_p$ norms with $p \geq 1$, components $\theta$ with larger $|\theta|^p$ give a larger contribution to the norm, so assuming for example's sake that two components $\theta_1$ and $\theta_2$ have the same effect on the fit of the model and $|\theta_1|^p > |\theta_2|^p > 1$, the minimization will try to reduce the size of $\theta_1$ more than $\theta_2$. Conversely, for $p > 1$, any $\theta_j$ with $|\theta_j|^p < 1$ will not have its size reduced very much at all, irrespective of the amount to which it contributes to minimizing the error of the model.

\subsubsection{Applying regularization to logistic regression}

$\ell_1$ regularization is achieved by adding the term $\lambda {\norm{\vect{w}}_1}^2$ where $\norm{\vect{w}}_1 = \sum_{i=1}^{c} |\vect{w}_i|$ (Theodoridis 2015, 404), so

\begin{equation*}
  NLL(\vect{w}, \lambda) = \log(1 + \exp(-y_i \vect{w}^T\vect{x})) + \lambda {\norm{\vect{w}}_1}
\end{equation*}

$\ell_2$ regularization is achieved by adding the term $\frac{\lambda}{2} {\norm{w}_2}^2$ to $NLL(\vect{w})$ above, giving

\begin{equation*}
  NLL(\vect{w}, \lambda) = \log(1 + \exp(-y_i \vect{w}^T\vect{x})) + \frac{\lambda}{2} {\norm{\vect{w}}_2}^2
\end{equation*}

However, for $p = 1$, even components $\theta_j$ with $|\theta_j|^1 < 1$ will have the regularization applied to them. Therefore irrespective of the size of a true $\theta_j$, the regularization will force $\theta_j$ to 0 if it does not contribute to minimizing model error.

(The above discussion was based Theodoridis 2015, 406-407)

\begin{figure}[ht!]
\centering
\includegraphics[width=90mm]{figure1.jpg}
\caption{The approximate effect of the $\ell_p$ norm on a given\\ component of the parameter vector \label{overflow}}
\end{figure}

\subsection{Kernels: linear vs RBF}

Kernels are commonly used to model similarities over pairs of data points. A Mercer kernel is a kernel whose Gram matrix

\begin{equation*}
  \vect{K} = \begin{pmatrix}
    \kappa(\vect{x}_1, \vect{x}_1) & \cdots & \kappa(\vect{x}_1, \vect{x}_N) \\
    & \vdots & \\
    \kappa(\vect{x}_N, \vect{x}_1) & \cdots & \kappa(\vect{x}_N, \vect{x}_N) \\    
  \end{pmatrix}
\end{equation*}

is positive semi-definite for any set of inputs $\{\vect{x}_i\}_{i=1}^N$ (Murphy 2012, 481). For any Mercer kernel there exists a function $\vect{\phi}: \mathcal{X} \rightarrow \mathbb{R}^D$ for which then $K(\vect{x}, \vect{x}^\prime) = \vect{\phi}(\vect{x})^T\vect{\phi}(\vect{x})$. Note that $D$ can be infinite, as explained in the section ``SVM and RBF kernel relationship explanation.''

In this project we use two kernels, linear kernels and the RBF kernel, both of which are Mercer kernels. The kernels will be used in this project as transformations of data to be input to classifiers which produce a linear decision boundary (if transformed data is input to a classifier, the resulting decision boudary will be linear in that transformed space).

Note that usually it is hard to derive the feature vector $\vect{\phi}(\vect{x})$ from a Kernel $\kappa(\vect{x}, \vect{x}^\prime)$, but the reverse is not difficult for a Mercer kernel since $\kappa(\vect{x}, \vect{x}^\prime) = \vect{\phi}(\vect{x})$.

The linear kernel is $\kappa(\vect{x}, \vect{x}^\prime) = \vect{x}^T\vect{x}^\prime$, which corresponds to the case where $\vect{\phi}(\vect{x}) = \vect{x}$, so $\vect{\phi}(\vect{x})$ takes points in $\mathcal{X}$ to $\mathcal{X}$. This kernel is useful in the case where the decision boundary is linear in the original feature space, so transforming them to a higher-dimensional feature space is not necessary (Murphy 2012, 482).

The RBF kernel is defined as follows:

\begin{equation*}
  K(\vect{x}, \vect{x}^\prime) = \exp\left(-\gamma \norm{\vect{x} - \vect{x}^\prime}\right)
\end{equation*}

As noted above, the $D$ in $\vect{\phi}(\vect{x}): \mathcal{X} \rightarrow \mathbb{R}^D$ is infinite in the case of the RBF kernel. To understand the transformation, following Abu-Mostafa et al. (8-37), let $\gamma = 1$ and $\vect{x}$ be a scalar. Then

\begin{align*}
  K(x, x^\prime) & = \exp\left(-\norm{x - x^\prime}^2\right) \\
  & = \exp\left(-(x)^2\right) \cdot \exp\left(2xx^\prime\right) \cdot \exp(-\left(x^\prime\right)^2) \\
  & = \exp\left(-(x)^2\right) \cdot \left(\sum_{k=0}^{\infty} \frac{2^k(x)^k\left(x^\prime\right)^k}{k!}\right) \cdot \exp\left(-\left(x^\prime\right)^2\right)
\end{align*}

Defining

\begin{equation*}
  \vect{\phi}(x) = \exp(-x^2) \cdot \left(1, \sqrt{\frac{2^1}{1!}}x, \sqrt{\frac{2^1}{2!}}x^2, \sqrt{\frac{2^1}{3!}}x^3, \ldots \right)
\end{equation*}

we see that $K(x, x^\prime) = \vect{\phi}(x)^T \vect{\phi}(x)$. The right hand side is an inner product in an infinite-dimensional feature space, which shows that the $D$ in the range of $K$ can be infinite.

\subsubsection{The ``kernel trick''}

If it is difficult to compute $\vect{\phi}(\vect{x})^T \vect{\phi}(\vect{x})$, instead we can compute $K(\vect{x}, \vect{x}^\prime)$ in the original $\mathcal{X}$ space since the results are equal. For the kernels used in this project, this is useful for the RBF kernel, as exact calculation of $\vect{\phi}(\vect{x})^T \vect{\phi}(\vect{x})$ in the range space of $\vect{\phi}$ is impossible. 

\subsection{SVMs}

The SVM is a classifier that incorporates sparsity of data points (as opposed to features) into its loss function (Murphy 2012, 497). SVMs for classification use a loss function called hinge loss, which is of the form $L_\text{hinge}(y, \eta) = \max(0, 1 - y \eta) = (1 - y\eta)_{+}$ where $\eta = f(\vect{x})$ is the ``confidence'' (not necessarily a probability) in choosing label $y = 1$ (Murphy 2012, 499). The objective function is

\begin{equation*}
  \min_{\vect{w}, w_0} \frac{1}{2}{\norm{\vect{w}}_2}^2 + C \sum_{i=1}^{N}(1 - y_i f(\vect{x}_i))_{+}
\end{equation*}

This is non-differentiable, but by introducing slack variables, the minimization problem can be transformed to one solvable by quadratic programming (Murphy 2012, 499).

\subsubsection{Generalization and the large-margin principle}

The minimization problem mentioned in the previous paragram can be obtained through a different approach, namely maximizing the size of the margin $f(\vect{x}) / \norm{\vect{w}}_2$. This approach also depends on the introduction of slack variables which allows the problem to handle certain cases. The resulting objective function is the same as the approach from minimizing the hinge loss function.

The importance of the large-margin is that it helps the model's generalization performance (Theodoridis 2015, 550). An intuitive way to see this is by Figure 2 (this is Figure 14.11 from Murphy 2012, 500).

\subsubsection{Generalization and support vectors}

The solution for the weights for the SVM has the form $\widehat{\vect{w}} = \sum_i \alpha_i \vect{x}_i$ where $\vect{\alpha}$ has many entries equal to 0; the $\vect{x}_i$ corresponding to non-zero $\alpha_i$ are called support vectors. Since the parameter vector for the fitted SVM depends only on a subset of data points, this helps model generalizability (Theodoridis and Koutroumbas 2009, 206).

\begin{figure}[ht!]
\centering
\includegraphics[width=120mm]{largeMarginPrinciple2.jpg}
\caption{Visualization of the large margin principle \label{overflow}}
\end{figure}

The SVM is used in this project with both the linear and RBF kernels.

\subsection{SVM and logistic regression with the RBF kernel: a close relationship}

In this section, we explain the effects of using logistic regression on data that has been transformed with the RBF kernel, and how this relates to the case where an SVM is used with such transformed data.

The optimal $f(\vect{x})$ in fitting an SVM is of the form $f(\vect{x}) = \sum_{i=1}^{n} \alpha_i K(\vect{x}, \vect{x}_i^\prime)$ (Zhu and Hastie 2002, 186). Also since the negative log-likelihood (NLL) for logistic regression has a similar shape to the NLL of the SVM, replacing the NLL of the SVM with the NLL of the logistic regression gives roughly the same solution (Zhu and Hastie 2002, 186). Then for a Mercer kernel, the interpretation of the probability $p(\vect{x})$ (which equals $P(y = 1 | \vect{X} = \vect{x})$, Lin 2002) is

\begin{align*}
  p(\vect{x}) & = \frac{e^{f(\vect{x})}}{1 + e^{f(\vect{x})}} = \frac{1}{1 + \exp({-f(\vect{x})})} & \\
  & = \frac{1}{1 + \exp(-\sum_{i=1}^{n} \alpha_i K(\vect{x}, \vect{x}_i^\prime))} & \text{plugging in the optimal solution} \\
  & = \frac{1}{1 + \exp(-\sum_{i=1}^{n} \alpha_i \vect{\phi}(\vect{x}_i)^T \vect{\phi}(\vect{x}))} & \text{using the kernel trick} \\
  & = \frac{1}{1 + \exp(-\vect{w}^T \vect{\phi}(\vect{x}))} &
\end{align*}

where the last step is by defining $\vect{w} = \sum_{i} \alpha_i \vect{\phi}(\vect{x}_i)$ is the weighted sum of transformed support vectors. The last two steps here were taken from Guestrin (2007). This implies that the kernel trick can be used to run logistic regression on data that has been transformed to an infinite-dimensional feature space using the $\vect{\phi}$ corresponding to the RBF kernel.

\subsection{Summary of model fitting strategies and data transformations}

The following table summarizes the combinations of model fitting strategies and data transformations used in this project. Each column indicates a different model fitting strategy (used in conjunction, of course, with minimizing model error as represented by a loss function), while each row indicates kernel, in other words, a feature transformation. Each cell indicates the classifier that was used in conjuction with the fitting strategy and data transformation. Note that the model used for any particular combination is deterministic: in other words, the desired model fitting strategy and data transformation indicate a model choice.

\begin{table}[h]
  \centering
  \begin{tabular}{l|c|c|c|c|}
         & Simple loss function & \begin{tabular}{@{}c@{}}Loss function with \\ $\ell_1$ regularization\end{tabular} & \begin{tabular}{@{}c@{}}Loss function with \\ $\ell_2$ regularization\end{tabular} & \begin{tabular}{@{}c@{}}Few data points \&\\ large margin\end{tabular} \\
\hline
  Linear &         & \begin{tabular}{@{}c@{}}Logistic \\ regression\end{tabular}  & \begin{tabular}{@{}c@{}}Logistic \\ regression\end{tabular}  & SVM \\
\hline
  RBF    & \begin{tabular}{@{}c@{}}Logistic \\ regression\end{tabular} &          &          & SVM \\
\hline
  \end{tabular}
  \caption{Models used with different fitting strategies and \\ feature transformations}
  \end{table}

\section{Datasets}

The following subsections describe the datasets used, and they are followed by a summary table.

\subsection{Breast Cancer Dataset}
\begin{enumerate}
    \item Wisconsin Diagnostic Breast Cancer (WDBC)
    \item Source Information
    \begin{itemize}
        \item Creators:
        \begin{itemize}
            \item Dr. William H. Wolberg, General Surgery Dept., University of Wisconsin,  Clinical Sciences Center, Madison, WI 53792. wolberg@eagle.surgery.wisc.edu
            \item W. Nick Street, Computer Sciences Dept., University of Wisconsin, 1210 West Dayton St., Madison, WI 53706. street@cs.wisc.edu  608-262-6619
            \item Olvi L. Mangasarian, Computer Sciences Dept., University of Wisconsin, 1210 West Dayton St., Madison, WI 53706. olvi@cs.wisc.edu
        \end{itemize}
        \item Donor: Nick Street
        \item Date: November 1995
    \end{itemize}
    \item Relevant information
    \begin{itemize}
        \item Features are computed from a digitized image of a fine needle aspirate (FNA) of a breast mass.  They describe characteristics of the cell nuclei present in the image. A few of the images can be found at http://www.cs.wisc.edu/~street/images/
    \end{itemize}
    \item Number of instances: 569
    \item Number of attributes: 32 (ID, diagnosis, 30 real-valued input features)
    \item Attribute information:
    \begin{enumerate}
        \item ID number
        \item Diagnosis (M = malignant, B = benign)
        \item 3-32)
        \begin{itemize}
            \item Ten real-valued features are computed for each cell nucleus such as radius, texture, etc.
        \end{itemize}
        \item Class distribution: 357 benign, 212 malignant
    \end{enumerate}
    \item W.N. Street et al. (1993)
\end{enumerate}

\subsection{Letter Dataset}
\begin{enumerate}
    \item Letter Image Recognition Data
    \item Source Information
    \begin{itemize}
        \item Creator: David J. Slate
        \item Odesta Corporation; 1890 Maple Ave; Suite 115; Evanston, IL 60201
        \item Donor: David J. Slate (dave@math.nwu.edu) (708) 491-3867
        \item Date: January, 1991
    \end{itemize}
    \item Past Usage:
    \begin{itemize}
        \item P.W. Frey and D. J. Slate (Machine Learning Vol 6 \#2 March 91):
        \begin{itemize}
            \item "Letter Recognition Using Holland-style Adaptive Classifiers".
        \end{itemize}
    \end{itemize}
    \item Number of instances: 20000.
    \begin{itemize}
        \item However, we only used 1543 since we are testing binary classifiers and we chose the letters E and F, whose counts are listed below.
        \item The data was split 60/40 for a training/testing set.
    \end{itemize}
    \item Number of Attributes: 17 (Letter category and 16 numeric features)
    \item Attribute Information:
    \begin{itemize}
        \item lettr	capital letter	(26 values from A to Z)
        \item x-box	horizontal position of box	  (integer)
        \item y-box	vertical position of box	  (integer)
        \item width	width of box			      (integer)
        \item high 	height of box			      (integer)
        \item onpix	total \# on pixels		      (integer)
        \item x-bar	mean x of on pixels in box	  (integer)
        \item y-bar	mean y of on pixels in box	  (integer)
        \item x2bar	mean x variance			      (integer)
        \item y2bar	mean y variance			      (integer)
        \item xybar	mean x y correlation		  (integer)
        \item x2ybr	mean of x * x * y		      (integer)
        \item xy2br	mean of x * y * y		      (integer)
        \item x-ege	mean edge count left to right (integer)
        \item xegvy	correlation of x-ege with y	  (integer)
        \item y-ege	mean edge count bottom to top (integer)
        \item yegvx	correlation of y-ege with x	  (integer)
    \end{itemize}
    \item Class Distribution:
    \begin{itemize}
        \item 789-A 766-B 736-C 805-D \textbf{768-E} \textbf{775-F} 773-G
        \item 734-H 755-I 747-J 739-K 761-L 792-M 783-N
        \item 753-O 803-P 783-Q 758-R 748-S 796-T 813-U
        \item 764-V 752-W 787-X 786-Y 734-Z
    \end{itemize}
    \item P.W. Fray and D.J. Slate (1991)
\end{enumerate}

\subsection{Leukemia Dataset}
\begin{enumerate}
    \item This dataset set was built to help predict new classes of cancer. The original dataset was built from 38 leukemia bone marrow samples. The bone marrow samples were obtained from acute leukemia patients at diagnosis.
    \item Number of samples: 38
    \item Number of features: 7129
    \item Number of classes: 2. Acute myeloid leukemia (AML) \& Acute lymphoblastic leukemia (ALL)
    \item Training/Testing: The data was split 60/40
    \item Golub et al. (1999)
\end{enumerate}

\subsection{MNIST Dataset}
\begin{enumerate}
    \item The MNIST database is a large collection of handwritten digits from 0 to 9. This dataset is a subset of the larger dataset NIST (Grother, 1995), which contains 62 classes of handwritten characters. Each sample is a 28x28 black and white image.
    \item We represent each sample as a 1-dimensional vector with 784 features, each feature corresponding to a particular pixel. Each feature is an integer value between 0 and 255.
    \item Since we are testing binary classifiers, we only consider two digits out of the ten in the dataset. We chose "8" and "0" because they have many overlapping features, making them more difficult to classify. After a 60/40 train/test split, the resulting dataset has ~8000 training samples and ~5500 test samples with an approximately equal amount of samples representing each class.
    \item LeCun et al. (1998)
\end{enumerate}

\subsection{Wilt Dataset}
\begin{enumerate}
    \item Was created from  satellite images taken over the forest in Japan that contains Japanese Oak Wilt and Japanese Pine Wilt trees B.A. Johnson et al (2013). The images were used to determine whether or not the trees were diseased, which means the color of their canopy was more on the red side than the green side B.A. Johnson et al (2013).
    \item Features (6 total, 5 features, 1 class per sample):
    \begin{itemize}
        \item class: "w" (diseased trees), "n" (all other land cover)
        \item GLCM\_Pan: GLCM mean texture (Pan band)
        \item Mean\_G: Mean green value
        \item Mean\_R: Mean red value
        \item Mean\_NIR: Mean NIR value
        \item SD\_Pan: Standard deviation (Pan band)
    \end{itemize}
    \item Number of classes: 2
    \item Number of samples:
    \begin{itemize}
        \item Training set: 4339
        \item Testing set: 500
    \end{itemize}
    \item B.A. Johnson et al. (2013)
\end{enumerate}

\subsection{Summary of datasets}

The following table shows the number of training samples in each class (A or B), for each dataset:

\begin{table}[h]
  \centering
  \begin{tabular}{r|c c c}
    Dataset       & Features & Train [classA, classB] & Test [classA, classB] \\
    \hline
    Wilt          & 5        & [74, 4265]             & [187, 313] \\
    MNIST         & 784      & [4132, 4104]           & [2771, 2721] \\
    Leukemia      & 7129     & [27, 11]               & [20, 14] \\
    Letters       & 16       & [455, 470]             & [313, 305] \\
    Breast Cancer & 30       & [132, 209]             & [80, 148]
\end{tabular}
  \caption{Number of features and number of samples in \\classes A and B by dataset}
\end{table}

\section{Experimental analysis}
\begin{center}
  \begin{tabular}{r|c c c c c}
     Model & Breast Cancer & MNIST & Leukemia & Letters & Wilt \\
     \hline
      Logistic Regression $\ell_1$ & 0.0351 & 0.0133 & 0.0000 & 0.0146 & 0.3080 \\
      Logistic Regression $\ell_2$ & 0.0351 & 0.0127 & 0.0178 & 0.0146 & 0.3080 \\
      Logistic Regression RBF      & 0.0351 & 0.0057 & 0.0294 & 0.0113 & 0.1340 \\
      SVM RBF                      & 0.0236 & 0.0038 & 0.0294 & 0.0049 & 0.1440 \\
      SVM Linear                   & 0.0439 & 0.0144 & 0.0294 & 0.0243 & 0.2960 \\
\end{tabular}
\end{center}

\section{Findings}



\section{Summary and future work}

\section{Group member contributions}

\section{Works cited}

\subsection{Works cited in main report}

Abu-Mostafa, Malik Magdon-Ismail and Hsuan-Tien Lin. 2012. \emph{Learning with Data}, e-Chapter 8 (``Support Vector Machines'') AMLBook.

Gretton, Arthur et al. 2012. "A Kernel Two-Sample Test." Journal of Machine Learning Research. Vol 13, p. 723-773.

Guestrin, Carlos. 2007. ``Support Vector Machines.'' Lecture slides for ``Machine Learning – 10701/15781`` at Carnegie Mellon University. 

Hsu, Chih-Wei et al. 2016. "A Practical Guide to Support Vector Classification." Department of Computer Science, National Taiwan University.

Lin, Yi. 2002. ``Support Vector Machines and the Bayes Rule in Classification.'' \emph{Data Mining and Knowledge Discovery} (6): 259–275.

Murphy, Kevin. 2012. \emph{Machine Learning: A Probabilistic Perspective}. MIT Press: Cambridge, MA.

Theodoridis, Sergios and Konstantinos Koutroumbas. 2009. \emph{Pattern Recognition}. Academic Press: Burlington, MA.

Theodoridis, Sergios. 2015. \emph{Machine Learning: A Bayesian and Optimization Perspective}. Academic Press: London, United Kingdom.

Zhu, Ji and Trevor Hastie. 2004. ``Kernel Logistic Regression and the Import Vector Machine.'' \emph{Journal of Computational and Graphical Statistics}.  
Volume 14, 2005 - Issue 1.

\subsection{Works cited in literature review}

Aizerman, M. A., Emmanuel M. Braverman and Rozoner, L. I. 1964. "Theoretical Foundations of the Potential Function Method in Pattern Recognition Learning". \emph{Automation and Remote Control}. 25: 821–837.

Boser, Bernhard E., Isabelle M. Guyon and Vladimir N. Vapnik. 1992. "A Training Algorithm for Optimal Margin Classifiers". \emph{Proceedings of the fifth annual workshop on Computational learning theory – COLT '92}. p. 144.

Cox, DR. 1958. "The regression analysis of binary sequences (with discussion)". \emph{J Roy Stat Soc B}. 20: 215–242.

Tychonoff, A.N. and V.Y. Arsenin. \emph{Solution of ill-posed problems}. Winston \& Sons: Washington, 1977.

\subsection{Datasets}

Brian Alan Johnson, Ryutaro Tateishi and Nguyen Thanh Hoan. 2013. "A hybrid pansharpening approach and multiscale object-based image analysis for mapping diseased pine and oak trees" International Journal of Remote Sensing, 34:20, 6969-6982. https://doi.org/10.1080/01431161.2013.810825

P. W. Frey and D. J. Slate. 1991. "Letter Recognition Using Holland-style Adaptive Classifiers". Machine Learning Vol 6 \#2. https://link.springer.com/article/10.1007/BF00114162

Golub, T. R., D. K. Slonim, P. Tamayo, C. Huard, M. Gaasenbeek, J. P. Mesirov, H. Coller, et al. 1999. "Molecular Classification of Cancer: Class Discovery and Class Prediction by Gene Expression Monitoring." Science 286 (5439): 531–527. doi:10.1126/science.286.5439.531.

Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner. 1998 "Gradient-based learning applied to document recognition." Proceedings of the IEEE, 86(11):2278-2324.
W.N. Street, W.H. Wolberg and O.L. Mangasarian. 1993. "Nuclear feature extraction for breast tumor diagnosis." IS\&T/SPIE 1993 International Symposium on Electronic Imaging: Science and Technology, volume 1905, pages 861-870.


\end{document}
