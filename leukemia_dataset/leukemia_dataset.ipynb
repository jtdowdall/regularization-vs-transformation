{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\""]},{"cell_type":"markdown","metadata":{},"source":["\n# Overview of dataset\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The data was obtained from [the Broad Institute](http://portals.broadinstitute.org/cgi-bin/cancer/publications/view/43) and is stored as follows:\n\n<table border=\"2\" cellspacing=\"0\" cellpadding=\"6\" rules=\"groups\" frame=\"hsides\">\n\n\n<colgroup>\n<col  class=\"org-left\" />\n\n<col  class=\"org-left\" />\n</colgroup>\n<thead>\n<tr>\n<th scope=\"col\" class=\"org-left\">Type of data</th>\n<th scope=\"col\" class=\"org-left\">File name</th>\n</tr>\n</thead>\n\n<tbody>\n<tr>\n<td class=\"org-left\">Training data</td>\n<td class=\"org-left\">`data_set_ALL_AML_train.txt`</td>\n</tr>\n\n\n<tr>\n<td class=\"org-left\">Training data class labels</td>\n<td class=\"org-left\">`ALL_vs_AML_train_set_38_sorted.cls`</td>\n</tr>\n\n\n<tr>\n<td class=\"org-left\">Testing data</td>\n<td class=\"org-left\">`data_set_ALL_AML_independent.txt`</td>\n</tr>\n\n\n<tr>\n<td class=\"org-left\">Testing data class labels</td>\n<td class=\"org-left\">`Leuk_ALL_AML.test.cls`</td>\n</tr>\n</tbody>\n</table>\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n# Cleaning the data\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["def clean_training_data():\n    clean_lines = []\n    with open(\"data_set_ALL_AML_train.txt\", \"r\") as f:\n        lines = f.readlines()\n        clean_lines = [l.rstrip('\\t\\n') for l in lines]\n\n    with open(\"data_set_ALL_AML_train_cleaned.txt\", \"w\") as f:\n        f.writelines('\\n'.join(clean_lines))\n\n\nclean_training_data()"]},{"cell_type":"markdown","metadata":{},"source":["\n# Loading the data\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import numpy, scipy, pandas\nimport sklearn\nimport re\n\ndef load_data(x_filename, y_filename):\n    df_x = pandas.read_csv(x_filename, sep=\"\\t\")\n    df_x = df_x.select(lambda x: not re.search('call\\.*', x), axis=1)\n    df_x = df_x.drop(['Gene Description', \n                      'Gene Accession Number'], axis=1)\n    df_x = df_x.T\n    x = df_x.values\n\n    with open(y_filename, \"r\") as fin:\n        data = fin.read().splitlines(True)\n    data = data[1].rstrip()\n\n    y = numpy.fromstring(data, sep=\" \")\n\n    return x, y\n\n\nx_train, y_train = load_data(\"data_set_ALL_AML_train_cleaned.txt\",\n                             \"ALL_vs_AML_train_set_38_sorted.cls\")\nx_test, y_test = load_data(\"data_set_ALL_AML_independent.txt\",\n                           \"Leuk_ALL_AML.test.cls\")\ny_test = y_test[1:]  # dataset has one additional 0 at beginning, \n                     # so remove it"]},{"cell_type":"markdown","metadata":{},"source":["\n# Run models\n\n"]},{"cell_type":"markdown","metadata":{},"source":["\n## Kernelized logistic regression with L2 and L1 regularization, logistic regression with L1 regularization\n\n"]},{"cell_type":"markdown","metadata":{},"source":["To choose the $\\gamma$ function of the RBF kernel (where $\\gamma = 1/(2\\sigma^2)$) we follow the heuristic choice mentioned in Gretton et al. (p. 748) of setting $\\sigma$ to equal the median distance between points of the training data.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import sklearn.linear_model\nimport sklearn.kernel_ridge\nimport sklearn.metrics.pairwise\nfrom sklearn.metrics import confusion_matrix\nfrom scipy.spatial.distance import cdist\nimport statistics\n\n#import tabulate\n\n# Calculate gamma as in Gretton et al.\nb = cdist(x_train, x_train).ravel()\ngamma = 1/(2 * pow(statistics.median(b), 2))\n\n#y_test_onehot = numpy.zeros((len(y_test), 2))\n#y_test_onehot[numpy.arange(len(y_test)), y_test.astype(int)] = 1\n\n# Calculate RBF kernel \nK      = sklearn.metrics.pairwise.rbf_kernel(x_train, x_train, gamma=gamma)\nK_test = sklearn.metrics.pairwise.rbf_kernel(x_test, x_train, gamma=gamma)\n\n# Fit kernelized logistic regression\n# (note that l2 regularization is applied by default)\nclf = sklearn.linear_model.LogisticRegression(solver='lbfgs')\nclf.fit(K, y_train)\nkernelized_l2_preds = clf.predict(K_test)\n\n# Fit kernelized logistic regression with l1 regularization\n# (note that liblinear solver used by default)\nclf = sklearn.linear_model.LogisticRegression(penalty='l1')\nclf.fit(K, y_train)\nkernelized_l1_preds = clf.predict(K_test)\n\n# Fit non-kernelized logistic regression with l1 regularization\nclf = sklearn.linear_model.LogisticRegression(penalty='l1')\nclf.fit(x_train, y_train)\nl1_preds = clf.predict(x_test)"]},{"cell_type":"markdown","metadata":{},"source":["\n## Evaluation of results\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.metrics import zero_one_loss\n\nkernelized_l2_er = zero_one_loss(y_test, kernelized_l2_preds)\nkernelized_l1_er = zero_one_loss(y_test, kernelized_l1_preds)\nl1_er = zero_one_loss(y_test, l1_preds)\n\nkernelized_l2_cm = confusion_matrix(y_test, kernelized_l2_preds)\nkernelized_l1_cm = confusion_matrix(y_test, kernelized_l1_preds)\nl1_cm = confusion_matrix(y_test, l1_preds)"]},{"cell_type":"markdown","metadata":{},"source":["\n## SVM\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from sklearn.svm import SVC\n#from sklearn.cross_validation import StratifiedKFold\n#from sklearn.grid_search import GridSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import GridSearchCV\n\n# This code is a modification of code at\n# http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/auto_examples/svm/plot_svm_parameters_selection.html\n\ndef find_svm_best_params(kernel_type):\n    C_range = 2. ** numpy.arange(-5, 15, 2)\n    gamma_range = 2. ** numpy.arange(-5, 3, 2)\n\n    param_grid = dict(gamma=gamma_range, C=C_range)\n\n    grid_search = GridSearchCV(SVC(kernel='linear'), param_grid, cv=5)\n    grid_search.fit(x_train, y_train)\n    bestparams = grid_search.best_params_\n    #results_dict = grid_search.cv_results_\n    #for param, score_mean, score_sd in zip(results_dict['params'],        \n    #results_dict['mean_test_score'], results_dict['std_test_score']):\n    #    print(param, round(score_mean, 4), round(score_sd, 4))\n    return bestparams\n\ndef estimate_svm(C, gamma, x_train, y_train, x_test):\n    our_svm = SVC(C=C, gamma=gamma)\n    our_svm.fit(x_train, y_train)\n    svm_preds = our_svm.predict(x_test)\n    svm_er = zero_one_loss(y_test, svm_preds)\n    svm_cm = confusion_matrix(y_test, svm_preds)\n    return svm_er, svm_cm\n\nbestparams_rbf = find_svm_best_params(\"RBF\")\nsvm_rbf_er, svm_rbf_cm = estimate_svm(bestparams_rbf['C'], \n                                      bestparams_rbf['gamma'],\n                                      x_train, y_train, x_test)\nbestparams_linear = find_svm_best_params(\"linear\")\nsvm_linear_er, svm_linear_cm = estimate_svm(bestparams_linear['C'], \n                                            bestparams_linear['gamma'],\n                                            x_train, y_train, x_test)"]},{"cell_type":"markdown","metadata":{},"source":["\n## Print results\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from IPython.display import HTML, display\nimport pandas\n\nresults_df = pandas.DataFrame(columns=('Model', 'Empirical error'))\n\nseries_index = [\"Model\", \"Empirical error\"]\nresults_df = results_df.append(pandas.Series([\"Kernelized L2\", \n                                             kernelized_l2_er], \n                                index=series_index),\n                  ignore_index=True)\n\nresults_df = results_df.append(pandas.Series([\"Kernelized L1\", \n                                              kernelized_l1_er], \n                                index=series_index),\n                  ignore_index=True)\n\nresults_df = results_df.append(pandas.Series([\"L1\", l1_er], \n                                index=series_index),\n                  ignore_index=True)\n\nresults_df = results_df.append(pandas.Series([\"SVM (RBF kernel)\", \n                                              svm_rbf_er], \n                                             index=series_index),\n                  ignore_index=True)\nresults_df = results_df.append(pandas.Series([\"SVM (Linear kernel)\", \n                                              svm_linear_er], \n                                             index=series_index),\n                  ignore_index=True)\n\n#display(HTML(results_df.to_html()))\n\ndisplay(results_df)"]},{"cell_type":"markdown","metadata":{},"source":["\n# References (move to separate file later)\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Gretton, Arthur et al. 2012. \"A Kernel Two-Sample Test.\" *Journal of Machine Learning Research*. Vol 13, p. 723-773.\n\nHsu, Chih-Wei et al. 2016. \"A Practical Guide to Support Vector Classification.\" Department of Computer Science, National Taiwan University.\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}