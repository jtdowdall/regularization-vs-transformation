#+TITLE: Leukemia dataset

* Overview of dataset

The data was obtained from [[http://portals.broadinstitute.org/cgi-bin/cancer/publications/view/43][the Broad Institute]] and is stored as follows:

| Type of data               | File name                            |
|----------------------------+--------------------------------------|
| Training data              | ~data_set_ALL_AML_train.txt~         |
| Training data class labels | ~ALL_vs_AML_train_set_38_sorted.cls~ |
| Testing data               | ~data_set_ALL_AML_independent.txt~   |
| Testing data class labels  | ~Leuk_ALL_AML.test.cls~              |

* Cleaning the data

#+BEGIN_SRC ipython
def clean_training_data():
    clean_lines = []
    with open("data_set_ALL_AML_train.txt", "r") as f:
        lines = f.readlines()
        clean_lines = [l.rstrip('\t\n') for l in lines]

    with open("data_set_ALL_AML_train_cleaned.txt", "w") as f:
        f.writelines('\n'.join(clean_lines))


clean_training_data()
#+END_SRC

* Loading the data

#+BEGIN_SRC ipython
  import numpy, scipy, pandas
  import sklearn
  import re

  def load_data(x_filename, y_filename):
      df_x = pandas.read_csv(x_filename, sep="\t")
      df_x = df_x.select(lambda x: not re.search('call\.*', x), axis=1)
      df_x = df_x.drop(['Gene Description', 
                        'Gene Accession Number'], axis=1)
      df_x = df_x.T
      x = df_x.values

      with open(y_filename, "r") as fin:
          data = fin.read().splitlines(True)
      data = data[1].rstrip()

      y = numpy.fromstring(data, sep=" ")

      return x, y


  x_train, y_train = load_data("data_set_ALL_AML_train_cleaned.txt",
                               "ALL_vs_AML_train_set_38_sorted.cls")
  x_test, y_test = load_data("data_set_ALL_AML_independent.txt",
                             "Leuk_ALL_AML.test.cls")
  y_test = y_test[1:]  # dataset has one additional 0 at beginning, 
                       # so remove it
#+END_SRC

#+RESULTS:
: # Out[9]:

* Run models

To choose the $\gamma$ function of the RBF kernel (where $\gamma = 1/(2\sigma^2)$) we follow the heuristic choice mentioned in Gretton et al. (p. 748) of setting $\sigma$ to equal the median distance between points of the training data.

#+BEGIN_SRC ipython
import sklearn.linear_model
import sklearn.kernel_ridge
import sklearn.metrics.pairwise
from sklearn.metrics import confusion_matrix
from scipy.spatial.distance import cdist
import statistics

# Calculate gamma as in Gretton et al.
b = cdist(x_train, x_train).ravel()
gamma = 1/(2 * pow(statistics.median(b), 2))

#y_test_onehot = numpy.zeros((len(y_test), 2))
#y_test_onehot[numpy.arange(len(y_test)), y_test.astype(int)] = 1

# Calculate RBF kernel 
K      = sklearn.metrics.pairwise.rbf_kernel(x_train, x_train, gamma=gamma)
K_test = sklearn.metrics.pairwise.rbf_kernel(x_test, x_train, gamma=gamma)

# Fit kernelized logistic regression
# (note that l2 regularization is applied by default)
clf = sklearn.linear_model.LogisticRegression(solver='lbfgs')
clf.fit(K, y_train)
kernelized_l2_preds = clf.predict(K_test)

# Fit kernelized logistic regression with l1 regularization
# (note that liblinear solver used by default)
clf = sklearn.linear_model.LogisticRegression(penalty='l1')
clf.fit(K, y_train)
kernelized_l1_preds = clf.predict(K_test)

# Fit non-kernelized logistic regression with l1 regularization
clf = sklearn.linear_model.LogisticRegression(penalty='l1')
clf.fit(x_train, y_train)
l1_preds = clf.predict(x_test)
#+END_SRC

(TODO: evaluate the above results)

(TODO: add grid search for SVM parameters)

* References (move to separate file later)

Gretton, Arthur et al. 2012. "A Kernel Two-Sample Test." /Journal of Machine Learning Research/. Vol 13, p. 723-773.
